{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* original dataset: https://www.kaggle.com/competitions/fake-news/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# timing utilities\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./data/\"\n",
    "output_path = \"./models/\"\n",
    "\n",
    "submit = pd.read_csv(input_path + \"submit.csv\") # sample labels; I think it's just to show format of output for test predictions\n",
    "test = pd.read_csv(input_path + \"test.csv\")\n",
    "train = pd.read_csv(input_path + \"train.csv\")\n",
    "\n",
    "# labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# only run to download resources; can specify download directory with argument `download_dir`\n",
    "# import nltk\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, text, stop_words=True, lemmatize=True):\n",
    "    \"\"\"Clean text column of a dataframe.\n",
    "    \n",
    "    Remove special characters and stop words. Lemmatize words to common root.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame) : Dataframe with a text column to clean.\n",
    "        text (str) : Name of text column in `df`.\n",
    "        stop_words (bool, default True) : If True, remove English stopwords (\"the\", \"an\", etc.).\n",
    "        lemmatize (bool, default True) : If True, lemmatize words to common root.\n",
    "    \n",
    "    Returns:\n",
    "        (DataFrame) : The original `df` with `text` column replaced with cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # basic error handling\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna(subset=[text])\n",
    "    \n",
    "    # elminating weird characters\n",
    "    # edited to also drop numbers for BERT\n",
    "    df[text] = df[text].str.replace(r'[^a-zA-Z]', ' ', regex=True)\n",
    "    df[text] = df[text].str.lower()\n",
    "    df = df.dropna(subset=[text])\n",
    "    \n",
    "    tokens = df[text].str.split()\n",
    "    \n",
    "    if stop_words:\n",
    "        sw = stopwords.words(\"english\")\n",
    "        tokens = tokens.apply(lambda row_words: [word for word in row_words if sw.count(word)==0])\n",
    "    \n",
    "    if lemmatize:\n",
    "#         stemmer = PorterStemmer()\n",
    "#         tokens = tokens.apply(lambda row_words: [stemmer.stem(word) for word in row_words])\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = tokens.apply(lambda row_words: [lemmatizer.lemmatize(word) for word in row_words])\n",
    "    \n",
    "    tokens = tokens.apply(lambda row_words: ' '.join(row_words))\n",
    "\n",
    "    df[text] = tokens\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = clean_text(train, \"text\", stop_words=False, lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean.text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud to view contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "real_text = train_clean.loc[train_clean.label==0, \"text\"]\n",
    "real_text = \" \".join([article for article in real_text.astype(str)])\n",
    "\n",
    "fake_text = train_clean.loc[train_clean.label==1, \"text\"]\n",
    "fake_text = \" \".join([article for article in fake_text.astype(str)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Cloud of Real Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_wc = WordCloud().generate(real_text)\n",
    "plt.figure(figsize=(40, 20))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(real_wc, interpolation='bilinear')\n",
    "plt.title(\"Word Cloud of Real Articles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Cloud of Fake Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_wc = WordCloud().generate(fake_text)\n",
    "plt.figure(figsize=(40, 20))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(fake_wc, interpolation='bilinear')\n",
    "plt.title(\"Word Cloud of Fake Articles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "X = train_clean.text\n",
    "y = train_clean.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "start = timer()\n",
    "\n",
    "pipe_tfidf = Pipeline([(\"tfidf\", TfidfVectorizer()),\n",
    "                       (\"svd\", TruncatedSVD(random_state=42, n_components=4000)),\n",
    "                       (\"xgb\", XGBClassifier(random_state=42))]\n",
    "                     )\n",
    "params_tfidf = {\"tfidf__analyzer\": [\"word\"],\n",
    "                \"tfidf__ngram_range\": [(1, 1)],\n",
    "#                 \"svd__n_components\": [5000],\n",
    "                \"xgb__n_estimators\": [10, 50],\n",
    "                \"xgb__max_depth\": [6, 10]\n",
    "               }\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "gs_tfidf = GridSearchCV(pipe_tfidf, param_grid=params_tfidf, cv=5, scoring=scoring, refit=\"accuracy\")\n",
    "\n",
    "gs_tfidf.fit(X_train, y_train)\n",
    "\n",
    "end = timer()\n",
    "print(f\"Time elapsed: {timedelta(seconds=end-start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tfidf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_steps = gs_tfidf.best_estimator_.named_steps\n",
    "fitted_steps.svd.explained_variance_ratio_.sum() # a little low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_steps = gs_tfidf.best_estimator.named_steps\n",
    "pickle.dump(fitted_steps[\"tfidf\"], open(output_path + \"tfidf_vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(fitted_steps[\"svd\"], open(output_path + \"svd.pkl\", \"wb\"))\n",
    "pickle.dump(fitted_steps[\"xgb\"], open(output_path + \"xgb_with_tfidf.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "start = timer()\n",
    "\n",
    "sent_embedding_transformer = FunctionTransformer(lambda text: model.encode(text.tolist()))\n",
    "\n",
    "pipe_bert = Pipeline([(\"embedder\", sent_embedding_transformer),\n",
    "                  (\"xgb\", XGBClassifier(random_state=42))]\n",
    "                )\n",
    "\n",
    "params_bert = {\"xgb__n_estimators\": [10, 50], \n",
    "               \"xgb__max_depth\":  [6, 10]\n",
    "              }\n",
    "\n",
    "gs_bert = GridSearchCV(pipe_bert, params_bert, cv=5)\n",
    "\n",
    "gs_bert.fit(X_train, y_train)\n",
    "\n",
    "end = timer()\n",
    "print(f\"Time elapsed: {timedelta(seconds=end-start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_bert.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model.encode(train_clean.text.tolist()), open(output_path + \"train_sent_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(gs_bert.best_estimator_.named_steps[\"xgb\"], open(output_path + \"xgb_with_bert.pkl\", \"wb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I already have above things trained, I can just load them as in below (and **don't** fit, just transform):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open(output_path + \"tfidf_vectorizer.pkl\", \"rb\"))\n",
    "svd = pickle.load(open(output_path + \"svd.pkl\", \"rb\"))\n",
    "xgb_with_tfidf = pickle.load(open(output_path + \"svd.pkl\", \"rb\"))\n",
    "\n",
    "pipe_tfidf = Pipeline([(\"tfidf\", tfidf), (\"svd\", svd), (\"xgb\", xgb_with_tfidf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "sent_embedding_transformer = FunctionTransformer(lambda text: model.encode(text.tolist()))\n",
    "\n",
    "xgb_with_bert = pickle.load(open(output_path + \"xgb_with_bert.pkl\", \"rb\"))\n",
    "\n",
    "pipe_bert = Pipeline([(\"embeddings\", sent_embedding_transformer), (\"xgb\", xgb_with_bert)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit too small to see, but this is what the first tree looks like; also, here the features are the principle components\n",
    "##set up the parameters\n",
    "rcParams['figure.figsize'] = 80,50\n",
    "from xgboost import plot_tree\n",
    "\n",
    "plot_tree(xgb_with_tfidf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(xgb_with_bert)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "\n",
    "def get_scores(model, X_test, y_test):\n",
    "    \"\"\" Calculate fitted binary classifcation model performance with respect to various metrics.\n",
    "    \n",
    "    Helper function for sklearn.metrics.classification_report for convenience. Calculate fitted\n",
    "    binary classification model's Accuracy, Precision, Recall, F-1 score, and Support for both \n",
    "    classes in input data.\n",
    "    \n",
    "    Args:\n",
    "        model (sklearn.base.ClassifierMixin) : Fitted classifier that implements scikit-learn API. \n",
    "        X_test (array-like) : Data to produce predicted labels.\n",
    "        y_test (1d array-like) : Correct target values to compare with predicted labels. Label\n",
    "        values assumed to be 0 and 1, with 0 meaning \"Real\" and 1 meaning \"Fake\". Class 1 is the\n",
    "        \"positive\" class.\n",
    "        \n",
    "    Returns:\n",
    "        (dict) : Dictionary of score results.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = classification_report(y_test, y_pred, labels=[0, 1], target_names=[\"Real\", \"Fake\"],\n",
    "                                    digits=4, output_dict=True)\n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tfidf = get_scores(pipe_tfidf, X_test, y_test)\n",
    "results_bert = get_scores(pipe_bert, X_test, y_test)\n",
    "print(results_tfidf)\n",
    "print(results_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For BERT\n",
    "\n",
    "### This section is from when I was trying to use BERT for word embeddings, but it was too computationally expensive a task, and sentence embeddings might be better for a use case. Basically this is scratch work that can be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, a \"sentence\" is just a linguistic body of words, so I'm going to treat each article as a single sentence, because otherwise I'd have to draw a manual distinction between which half of the article is the first \"sentence\" and which is second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max token amount allowed is 512, I'll have to truncate to first few hundred words, but need leeway for words that aren't recognized so they're broken down into multiple subwords. For instance, I first tried truncating to first 500, but I ended up with 535 tokens.\n",
    "\n",
    "Following along with this: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#1-loading-pre-trained-bert"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Below is for manually making tensors for word embeddings, apparently it can also be done manually\n",
    "\n",
    "train_clean[\"first_300\"] = train_clean.text.str.split().apply(lambda x: x[:300]).apply(lambda x: ' '.join(x))\n",
    "# I don't need to actually do this line if i'm having tokenizer make a pytorch tensor\n",
    "# sample_text = \"[CLS] \" + train_clean.first_300[0] + \" [SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(sample_text)\n",
    "\n",
    "# index of each word in text with respect to BERT's original vocabulary\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# need a series of one to indicate which sentence every token is in. If two sentences, need to label 0 and 1\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "# Using pytorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensor = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "* want to play around with xgboost's xgb.train and DMatrix's because the XGBClassifier I'm using is just the sklearn wrapper\n",
    "* Try to make submission csv's using both sets of models\n",
    "\n",
    "* Taha has a few other suggestions\n",
    "    * transformers, specifically huggingface -> just pass data through, and it'll produce embeddings\n",
    "    * a bit about embeddings\n",
    "        * formed with artificual neural networks\n",
    "        * remove output layer, add two inner layers: an encoder and then decoder, they're both fully connected\n",
    "        * usually input layer isn't fully connected to first encoder inner layer, \n",
    "        * output is n-dimensional where n is how many nodes in decoder layer\n",
    "        * maybe truncate to first 500 words before doing this\n",
    "    * can use pre-trained models like BERT, gp3 something by Deepmind\n",
    "    * use examples here: https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens, and https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "    * consider also using sentence-level instead of word-level\n",
    "    * he's also mentioning topioc modeling\n",
    "        * bunch of text, want to extract groups of topics in corpus\n",
    "            * use this: https://github.com/MaartenGr/BERTopic\n",
    "\n",
    "* end product could be like a SaaS application where someone can input a news article and predict whether article is fake news\n",
    "* streamlit or plotly or flask for frontend\n",
    "    * in flask, create a file as a server\n",
    "* for a freecodecamp flask tutorial: https://www.youtube.com/watch?v=Z1RJmh_OqeA\n",
    "    * but only consider that stuff after I get basic model online"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
